import fs from 'node:fs/promises'
import path from 'node:path'
import { fileURLToPath } from 'node:url'
import { createHash } from 'node:crypto'
import matter from 'gray-matter'
import { glob } from 'glob'

const __dirname = path.dirname(fileURLToPath(import.meta.url))
const ROOT_DIR = path.resolve(__dirname, '..')
const DOCS_DIR = path.join(ROOT_DIR, 'docs')
const OUTPUT_DIR = path.join(ROOT_DIR, 'public')

// æ¯ä¸ªåˆ†å—åŒ…å«çš„æ–‡æ¡£æ•°é‡
const CHUNK_SIZE = 50

async function generateSearchIndex() {
  console.log('ğŸ” Generating search index...')

  try {
    // æŸ¥æ‰¾æ‰€æœ‰ markdown æ–‡ä»¶
    // windows è·¯å¾„å…¼å®¹æ€§å¤„ç†
    const pattern = `${DOCS_DIR.replace(/\\/g, '/')}/**/*.md`
    const files = await glob(pattern)

    const indexData = []

    for (const file of files) {
      const content = await fs.readFile(file, 'utf-8')
      const { data, content: body } = matter(content)

      // è·³è¿‡å¯†ç ä¿æŠ¤çš„æ–‡æ¡£ï¼Œä¸åŠ å…¥æœç´¢ç´¢å¼•
      if (data.password) {
        console.log(`  â­ï¸ Skipping protected document: ${path.basename(file)}`)
        continue
      }

      // ç”Ÿæˆç›¸å¯¹è·¯å¾„è·¯ç”±
      let clientRoute = path.relative(DOCS_DIR, file).replace(/\\/g, '/').replace(/\.md$/, '')
      if (clientRoute.endsWith('index')) clientRoute = clientRoute.slice(0, -5)

      // ç®€æ˜“å»é™¤ Markdown è¯­æ³•
      const plainText = body
        .replace(/!\[.*?\]\(.*?\)/g, '') // å›¾ç‰‡
        .replace(/\[.*?\]\(.*?\)/g, '$1') // é“¾æ¥
        .replace(/`{3}[\s\S]*?`{3}/g, '') // ä»£ç å—å†…å®¹é€šå¸¸ä¸ç´¢å¼•
        .replace(/`(.+?)`/g, '$1') // è¡Œå†…ä»£ç 
        .replace(/#+\s/g, '') // æ ‡é¢˜
        .replace(/>\s/g, '') // å¼•ç”¨
        .replace(/\*\*/g, '') // ç²—ä½“
        .replace(/\*/g, '') // æ–œä½“
        .replace(/\n+/g, ' ') // æ¢è¡Œå˜ç©ºæ ¼
        .trim()
        .slice(0, 5000) // é™åˆ¶é•¿åº¦

      indexData.push({
        id: clientRoute,
        title: data.title || path.basename(file, '.md'),
        content: plainText,
        tags: data.tags || []
      })
    }

    // ç”Ÿæˆç‰ˆæœ¬å“ˆå¸Œï¼ˆåŸºäºæ‰€æœ‰æ–‡æ¡£å†…å®¹ï¼‰
    const contentHash = createHash('md5')
      .update(JSON.stringify(indexData))
      .digest('hex')
      .slice(0, 8)

    // è®¡ç®—åˆ†å—æ•°é‡
    const totalChunks = Math.ceil(indexData.length / CHUNK_SIZE)

    // å†™å…¥åˆ†å—æ–‡ä»¶
    for (let i = 0; i < totalChunks; i++) {
      const start = i * CHUNK_SIZE
      const end = Math.min(start + CHUNK_SIZE, indexData.length)
      const chunk = indexData.slice(start, end)

      const chunkPath = path.join(OUTPUT_DIR, `search-index-${i}.json`)
      await fs.writeFile(chunkPath, JSON.stringify(chunk))
      console.log(`  ğŸ“¦ Chunk ${i}: ${chunk.length} documents`)
    }

    // å†™å…¥æ¸…å•æ–‡ä»¶
    const manifest = {
      version: contentHash,
      totalDocs: indexData.length,
      chunks: totalChunks,
      chunkSize: CHUNK_SIZE,
      generatedAt: new Date().toISOString()
    }
    await fs.writeFile(
      path.join(OUTPUT_DIR, 'search-index-manifest.json'),
      JSON.stringify(manifest, null, 2)
    )

    // åŒæ—¶ä¿ç•™å®Œæ•´ç´¢å¼•æ–‡ä»¶ï¼ˆå‘åå…¼å®¹ï¼‰
    await fs.writeFile(
      path.join(OUTPUT_DIR, 'search-index.json'),
      JSON.stringify(indexData, null, 2)
    )

    console.log(`âœ… Search index generated:`)
    console.log(`   - Version: ${contentHash}`)
    console.log(`   - Documents: ${indexData.length}`)
    console.log(`   - Chunks: ${totalChunks}`)
  } catch (error) {
    console.error('âŒ Failed to generate search index:', error)
    process.exit(1)
  }
}

generateSearchIndex()
